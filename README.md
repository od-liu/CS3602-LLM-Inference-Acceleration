# CS3602 LLM Inference Acceleration

CS3602å¤§ä½œä¸šï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„KV Cacheä¼˜åŒ–ä¸æ¨ç†åŠ é€Ÿã€‚

æœ¬é¡¹ç›®å®ç°äº†å¤šç§ KV Cache å‹ç¼©æ–¹æ³•ï¼Œç»Ÿä¸€åœ¨ `kvcompress` åº“ä¸­ç®¡ç†ï¼š

1. **L2 Compress (KnormPress)** - åŸºäº L2 èŒƒæ•°çš„æ¯”ä¾‹å‹ç¼©
2. **Fix-Size L2** - å›ºå®šå¤§å° KV Cache å‹ç¼©
3. **StreamingLLM** - åŸºäº Attention Sink çš„æµå¼å‹ç¼©

## é¡¹ç›®æ¦‚è¿°

KVCompress æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ KV Cache å‹ç¼©åº“ï¼Œæ”¯æŒå¤šç§å‹ç¼©ç­–ç•¥ï¼š

| æ–¹æ³• | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `l2_compress` | æŒ‰ `keep_ratio` æ¯”ä¾‹å‹ç¼©ï¼Œä¿ç•™ä½ L2 èŒƒæ•° token | é€šç”¨å‹ç¼© |
| `fix_size_l2_compress` | ç»´æŒå›ºå®š KV Cache å¤§å°ï¼Œæ”¯æŒå¤šç§é©±é€ç­–ç•¥ | å†…å­˜å—é™åœºæ™¯ |
| `streaming_llm_compress` | ä¿ç•™ attention sinks + æœ€è¿‘ tokens | æ— é™é•¿åº¦æµå¼è¾“å…¥ |

### StreamingLLM æ–¹æ³•

StreamingLLM æ˜¯æ¥è‡ª MIT Han Lab çš„æ–¹æ³•ï¼ˆICLR 2024ï¼‰ï¼Œæ ¸å¿ƒå‘ç°æ˜¯ï¼š

- LLM ä¼šå°†å¤§é‡ attention åˆ†é…ç»™åˆå§‹ tokensï¼ˆ"attention sinks"ï¼‰ï¼Œå³ä½¿å®ƒä»¬è¯­ä¹‰ä¸Šä¸é‡è¦
- é€šè¿‡ä¿ç•™è¿™äº› attention sinks + æ»‘åŠ¨çª—å£çš„æœ€è¿‘ tokensï¼Œå¯ä»¥å¤„ç†æ— é™é•¿åº¦çš„è¾“å…¥

```
Cache ç»“æ„: [initial tokens (0:start_size)] + [recent tokens (seq_len-recent_size:seq_len)]
é»˜è®¤é…ç½®: 4 initial tokens + 508 recent tokens = 512 total
```

## é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ README.md                    # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ LICENSE                      # è®¸å¯è¯
â”œâ”€â”€ results.txt                  # å®éªŒç»“æœæ—¥å¿—
â”‚
â”œâ”€â”€ docs/                        # ğŸ“š æ–‡æ¡£
â”‚   â”œâ”€â”€ lab-instruction.md       # ä½œä¸šè¦æ±‚
â”‚   â”œâ”€â”€ KnormPress.pdf           # KnormPress è®ºæ–‡
â”‚   â””â”€â”€ L2_COMPRESS_ANALYSIS.md  # å‹ç¼©æ•ˆæœåˆ†æ
â”‚
â”œâ”€â”€ data/                        # ğŸ“Š æ•°æ®é›†
â”‚   â””â”€â”€ pg19.parquet             # PG-19 é•¿æ–‡æœ¬æ•°æ®é›†
â”‚
â”œâ”€â”€ kvcompress/                  # ğŸ§  æ ¸å¿ƒå‹ç¼©åº“ â­
â”‚   â”œâ”€â”€ __init__.py              # ç»Ÿä¸€å¯¼å‡º
â”‚   â”œâ”€â”€ methods/                 # å‹ç¼©æ–¹æ³•
â”‚   â”‚   â”œâ”€â”€ __init__.py          # æ–¹æ³•æ³¨å†Œè¡¨
â”‚   â”‚   â”œâ”€â”€ base.py              # åŸºç±»å’Œæ¥å£
â”‚   â”‚   â”œâ”€â”€ l2_compress.py       # KnormPress L2 å‹ç¼©
â”‚   â”‚   â”œâ”€â”€ fix_size_l2.py       # å›ºå®šå¤§å° L2 å‹ç¼©
â”‚   â”‚   â””â”€â”€ streaming_llm.py     # StreamingLLM æ–¹æ³• â­
â”‚   â”œâ”€â”€ evaluate.py              # ç»Ÿä¸€è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ benchmark.py             # ç»Ÿä¸€åŸºå‡†æµ‹è¯•æ¨¡å—
â”‚   â””â”€â”€ utils.py                 # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ scripts/                     # ğŸ› ï¸ å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ benchmark.py             # ç»Ÿä¸€åŸºå‡†æµ‹è¯•å…¥å£ â­
â”‚   â””â”€â”€ plot_compression_results.py  # å¯è§†åŒ–ç»˜å›¾
â”‚
â”œâ”€â”€ baseline_test.py             # åŸºçº¿æ€§èƒ½æµ‹è¯•
â”‚
â””â”€â”€ results/                     # ğŸ“ˆ ç»“æœå›¾è¡¨
    â”œâ”€â”€ strategy_comparison.png      # ç­–ç•¥å¯¹æ¯”å›¾
    â”œâ”€â”€ keep_ratio_analysis.png      # Keep Ratio åˆ†æå›¾
    â”œâ”€â”€ ppl_accuracy_tradeoff.png    # PPL-Accuracy æƒè¡¡å›¾
    â”œâ”€â”€ improvement_summary.png      # æ”¹è¿›æ€»ç»“å›¾
    â””â”€â”€ compression_comparison.png   # å‹ç¼©æ•ˆæœå¯¹æ¯”å›¾
```

## ç¯å¢ƒé…ç½®

### ä¾èµ–å®‰è£…

```bash
# åˆ›å»ºå¹¶æ¿€æ´» conda ç¯å¢ƒ
conda create -n nlp python=3.11
conda activate nlp

# å®‰è£…ä¾èµ–
pip install torch transformers datasets numpy tqdm
```

### æ¨¡å‹å’Œæ•°æ®é›†

- **æ¨¡å‹**: `EleutherAI/pythia-2.8b`
- **æ•°æ®é›†**: `PG-19` (é•¿æ–‡æœ¬), `wikitext-2-raw-v1` (çŸ­æ–‡æœ¬)

## ä½¿ç”¨æ–¹æ³•

### 1. ç»Ÿä¸€åŸºå‡†æµ‹è¯•ï¼ˆæ¨èï¼‰

```bash
# æµ‹è¯• L2 å‹ç¼©ï¼ˆKnormPressï¼‰
python scripts/benchmark.py --method l2_compress --keep_ratios 1.0,0.8,0.5

# æµ‹è¯•å›ºå®šå¤§å° L2 å‹ç¼©
python scripts/benchmark.py --method fix_size_l2 --fix_kv_sizes 256,512 --strategies keep_low

# æµ‹è¯• StreamingLLM
python scripts/benchmark.py --method streaming_llm --start_size 4 --recent_sizes 252,508,1020

# å¯¹æ¯”æ‰€æœ‰æ–¹æ³•
python scripts/benchmark.py --compare_all
```

### 2. åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from kvcompress import (
    l2_compress, 
    fix_size_l2_compress, 
    streaming_llm_compress,
    evaluate_with_compression
)

# æ–¹æ³•1: L2 æ¯”ä¾‹å‹ç¼© (KnormPress)
compressed_kv = l2_compress(
    past_key_values,
    keep_ratio=0.8,      # ä¿ç•™ 80%
    prune_after=1000,    # è¶…è¿‡ 1000 token æ‰å‹ç¼©
    skip_layers=[0, 1]   # è·³è¿‡å‰ä¸¤å±‚
)

# æ–¹æ³•2: å›ºå®šå¤§å°å‹ç¼©
compressed_kv = fix_size_l2_compress(
    past_key_values,
    fix_kv_size=512,       # æœ€å¤šä¿ç•™ 512 token
    keep_ratio=0.2,        # æœ€è¿‘ 20% ä¸é©±é€
    strategy="keep_low",   # ä¿ç•™ä½èŒƒæ•° token
    skip_layers=[0, 1]
)

# æ–¹æ³•3: StreamingLLM
compressed_kv = streaming_llm_compress(
    past_key_values,
    start_size=4,          # ä¿ç•™ 4 ä¸ª attention sink tokens
    recent_size=508,       # ä¿ç•™æœ€è¿‘ 508 ä¸ª tokens
)

# ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°æ¥å£
results = evaluate_with_compression(
    model, tokenizer, text,
    compress_fn=streaming_llm_compress,
    compress_kwargs={"start_size": 4, "recent_size": 508}
)
print(f"PPL: {results['perplexity']:.2f}, Acc: {results['accuracy']:.2%}")
```

### 3. ä½¿ç”¨æ–¹æ³•æ³¨å†Œè¡¨

```python
from kvcompress import get_compress_fn, list_methods

# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ–¹æ³•
print(list_methods())  # ['l2_compress', 'fix_size_l2', 'streaming_llm']

# é€šè¿‡åç§°è·å–å‹ç¼©å‡½æ•°
compress_fn = get_compress_fn("streaming_llm")
compressed = compress_fn(past_key_values, start_size=4, recent_size=508)
```

## æ ¸å¿ƒç®—æ³•

### l2_compress (æ¯”ä¾‹å‹ç¼©)

```
è¾“å…¥: KV Cache (seq_len tokens), keep_ratio
è¾“å‡º: å‹ç¼©åçš„ KV Cache (seq_len * keep_ratio tokens)

1. è®¡ç®—æ¯ä¸ª token çš„ L2 èŒƒæ•°
2. æŒ‰èŒƒæ•°å‡åºæ’åº
3. ä¿ç•™å‰ keep_ratio æ¯”ä¾‹çš„ä½èŒƒæ•° token
4. æ¢å¤æ—¶é—´é¡ºåº
```

### fix_size_l2_compress (å›ºå®šå¤§å°)

```
è¾“å…¥: KV Cache, fix_kv_size, keep_ratio
è¾“å‡º: æœ€å¤š fix_kv_size tokens çš„ KV Cache

1. å¦‚æœ seq_len <= fix_kv_sizeï¼Œä¸å‹ç¼©
2. è®¡ç®—ä¿æŠ¤åŒºå¤§å°: protected = fix_kv_size * keep_ratio
3. é©±é€åŒº = å‰ (seq_len - protected) ä¸ª token
4. ä»é©±é€åŒºé€‰æ‹© (fix_kv_size - protected) ä¸ª token ä¿ç•™
5. åˆå¹¶: ä¿ç•™çš„é©±é€åŒº token + ä¿æŠ¤åŒº token
```

### streaming_llm_compress (StreamingLLM)

```
è¾“å…¥: KV Cache, start_size, recent_size
è¾“å‡º: æœ€å¤š (start_size + recent_size) tokens çš„ KV Cache

1. å¦‚æœ seq_len <= (start_size + recent_size)ï¼Œä¸å‹ç¼©
2. ä¿ç•™ attention sinks: tokens[0:start_size]
3. ä¿ç•™æœ€è¿‘ tokens: tokens[-recent_size:]
4. æ‹¼æ¥: attention sinks + recent tokens
```

## å®éªŒç»“æœ

### æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | ç‰¹ç‚¹ | PPL å½±å“ | é€Ÿåº¦ |
|------|------|----------|------|
| Baseline | æ— å‹ç¼© | åŸºå‡† | åŸºå‡† |
| L2 (keep_ratio=0.8) | ä¿ç•™ 80% é‡è¦ token | +2-5% | å¿« |
| Fix-Size (512, keep_low) | å›ºå®šå¤§å°ï¼Œæ™ºèƒ½é©±é€ | +5-10% | ä¸­ç­‰ |
| StreamingLLM (4+508) | Attention sinks + æ»‘åŠ¨çª—å£ | +3-8% | æœ€å¿« |

### å¯è§†åŒ–ç»“æœ

è¿è¡Œç»˜å›¾è„šæœ¬ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼š
```bash
python scripts/plot_compression_results.py
```

## å‚è€ƒæ–‡çŒ®

- **KnormPress**: [A Simple and Effective L2 Norm-Based Strategy for KV Cache Compression](https://arxiv.org/abs/2406.11430) (EMNLP 2024)
- **StreamingLLM**: [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) (ICLR 2024)
- **Pythia æ¨¡å‹**: [EleutherAI/pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped)

## æ€»ç»“

æœ¬é¡¹ç›®å®ç°äº†ç»Ÿä¸€çš„ KV Cache å‹ç¼©åº“ `kvcompress`ï¼š

âœ… **å¤šç§å‹ç¼©æ–¹æ³•**: l2_compress, fix_size_l2, streaming_llm  
âœ… **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„å‡½æ•°ç­¾å  
âœ… **æ–¹æ³•æ³¨å†Œè¡¨**: æ–¹ä¾¿æ‰©å±•æ–°æ–¹æ³•  
âœ… **ç»Ÿä¸€è¯„ä¼°**: æ”¯æŒ PPL, Accuracy, TTFT, TPOT  
âœ… **ç»Ÿä¸€åŸºå‡†æµ‹è¯•**: å•ä¸€è„šæœ¬æµ‹è¯•æ‰€æœ‰æ–¹æ³•

## ä½œè€…

Jiamin Liu

## è‡´è°¢

æ„Ÿè°¢ KnormPress å’Œ StreamingLLM è®ºæ–‡ä½œè€…æä¾›çš„å¼€æºå®ç°å’Œè¯¦ç»†æ–‡æ¡£ã€‚
